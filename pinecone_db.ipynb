{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.1.17)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.6.5)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.36 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.0.36)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.48 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (0.1.67)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.7.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.2.0,>=0.1.48->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-pinecone in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.1.1)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-pinecone) (0.1.52)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-pinecone) (1.26.4)\n",
      "Requirement already satisfied: pinecone-client<4.0.0,>=3.2.2 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-pinecone) (3.2.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (0.1.67)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (2.7.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.1.52->langchain-pinecone) (8.2.3)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (2024.2.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (4.11.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (2.2.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain-pinecone) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-pinecone) (3.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-pinecone) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain-pinecone) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain-pinecone) (2.18.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.64.1->pinecone-client<4.0.0,>=3.2.2->langchain-pinecone) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-pinecone) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.52->langchain-pinecone) (3.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-openai in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.1.7)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.46 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-openai) (0.1.52)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.24.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-openai) (1.25.2)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-openai) (0.7.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (0.1.67)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (2.7.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<0.3,>=0.1.46->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openai<2.0.0,>=1.24.0->langchain-openai) (4.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.24.0->langchain-openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.24.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.46->langchain-openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.3,>=0.1.46->langchain-openai) (3.10.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.46->langchain-openai) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>4->openai<2.0.0,>=1.24.0->langchain-openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tiktoken) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddingsS\n",
    "from langchain.llms import openai\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file PDFs already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFDirectoryLoader(\"PDFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='You Only Look Once:\\nUniﬁed, Real-Time Object Detection\\nJoseph Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗†\\nUniversity of Washington∗, Allen Institute for AI†, Facebook AI Research¶\\nhttp://pjreddie.com/yolo/\\nAbstract\\nWe present YOLO, a new approach to object detection.\\nPrior work on object detection repurposes classiﬁers to per-\\nform detection. Instead, we frame object detection as a re-\\ngression problem to spatially separated bounding boxes and\\nassociated class probabilities. A single neural network pre-\\ndicts bounding boxes and class probabilities directly from\\nfull images in one evaluation. Since the whole detection\\npipeline is a single network, it can be optimized end-to-end\\ndirectly on detection performance.\\nOur uniﬁed architecture is extremely fast. Our base\\nYOLO model processes images in real-time at 45 frames\\nper second. A smaller version of the network, Fast YOLO,\\nprocesses an astounding 155 frames per second while\\nstill achieving double the mAP of other real-time detec-\\ntors. Compared to state-of-the-art detection systems, YOLO\\nmakes more localization errors but is less likely to predict\\nfalse positives on background. Finally, YOLO learns very\\ngeneral representations of objects. It outperforms other de-\\ntection methods, including DPM and R-CNN, when gener-\\nalizing from natural images to other domains like artwork.\\n1. Introduction\\nHumans glance at an image and instantly know what ob-\\njects are in the image, where they are, and how they inter-\\nact. The human visual system is fast and accurate, allow-\\ning us to perform complex tasks like driving with little con-\\nscious thought. Fast, accurate algorithms for object detec-\\ntion would allow computers to drive cars without special-\\nized sensors, enable assistive devices to convey real-time\\nscene information to human users, and unlock the potential\\nfor general purpose, responsive robotic systems.\\nCurrent detection systems repurpose classiﬁers to per-\\nform detection. To detect an object, these systems take a\\nclassiﬁer for that object and evaluate it at various locations\\nand scales in a test image. Systems like deformable parts\\nmodels (DPM) use a sliding window approach where the\\nclassiﬁer is run at evenly spaced locations over the entire\\nimage [10].\\nMore recent approaches like R-CNN use region proposal\\n1. Resize image.\\n2. Run convolutional network.3. Non-max suppression.\\nDog: 0.30Person: 0.64Horse: 0.28Figure 1: The YOLO Detection System. Processing images\\nwith YOLO is simple and straightforward. Our system (1) resizes\\nthe input image to 448×448, (2) runs a single convolutional net-\\nwork on the image, and (3) thresholds the resulting detections by\\nthe model’s conﬁdence.\\nmethods to ﬁrst generate potential bounding boxes in an im-\\nage and then run a classiﬁer on these proposed boxes. After\\nclassiﬁcation, post-processing is used to reﬁne the bound-\\ning boxes, eliminate duplicate detections, and rescore the\\nboxes based on other objects in the scene [13]. These com-\\nplex pipelines are slow and hard to optimize because each\\nindividual component must be trained separately.\\nWe reframe object detection as a single regression prob-\\nlem, straight from image pixels to bounding box coordi-\\nnates and class probabilities. Using our system, you only\\nlook once (YOLO) at an image to predict what objects are\\npresent and where they are.\\nYOLO is refreshingly simple: see Figure 1. A sin-\\ngle convolutional network simultaneously predicts multi-\\nple bounding boxes and class probabilities for those boxes.\\nYOLO trains on full images and directly optimizes detec-\\ntion performance. This uniﬁed model has several beneﬁts\\nover traditional methods of object detection.\\nFirst, YOLO is extremely fast. Since we frame detection\\nas a regression problem we don’t need a complex pipeline.\\nWe simply run our neural network on a new image at test\\ntime to predict detections. Our base network runs at 45\\nframes per second with no batch processing on a Titan X\\nGPU and a fast version runs at more than 150 fps. This\\nmeans we can process streaming video in real-time with\\nless than 25 milliseconds of latency. Furthermore, YOLO\\nachieves more than twice the mean average precision of\\nother real-time systems. For a demo of our system running\\nin real-time on a webcam please see our project webpage:\\nhttp://pjreddie.com/yolo/ .\\nSecond, YOLO reasons globally about the image when\\n1arXiv:1506.02640v5  [cs.CV]  9 May 2016', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='You Only Look Once:\\nUniﬁed, Real-Time Object Detection\\nJoseph Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗†', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='University of Washington∗, Allen Institute for AI†, Facebook AI Research¶\\nhttp://pjreddie.com/yolo/\\nAbstract', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='Abstract\\nWe present YOLO, a new approach to object detection.\\nPrior work on object detection repurposes classiﬁers to per-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='form detection. Instead, we frame object detection as a re-\\ngression problem to spatially separated bounding boxes and', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='associated class probabilities. A single neural network pre-\\ndicts bounding boxes and class probabilities directly from', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='full images in one evaluation. Since the whole detection\\npipeline is a single network, it can be optimized end-to-end', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='directly on detection performance.\\nOur uniﬁed architecture is extremely fast. Our base\\nYOLO model processes images in real-time at 45 frames', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='per second. A smaller version of the network, Fast YOLO,\\nprocesses an astounding 155 frames per second while', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='still achieving double the mAP of other real-time detec-\\ntors. Compared to state-of-the-art detection systems, YOLO', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='makes more localization errors but is less likely to predict\\nfalse positives on background. Finally, YOLO learns very', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='general representations of objects. It outperforms other de-\\ntection methods, including DPM and R-CNN, when gener-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='alizing from natural images to other domains like artwork.\\n1. Introduction\\nHumans glance at an image and instantly know what ob-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='jects are in the image, where they are, and how they inter-\\nact. The human visual system is fast and accurate, allow-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='ing us to perform complex tasks like driving with little con-\\nscious thought. Fast, accurate algorithms for object detec-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='tion would allow computers to drive cars without special-\\nized sensors, enable assistive devices to convey real-time', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='scene information to human users, and unlock the potential\\nfor general purpose, responsive robotic systems.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='Current detection systems repurpose classiﬁers to per-\\nform detection. To detect an object, these systems take a', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='classiﬁer for that object and evaluate it at various locations\\nand scales in a test image. Systems like deformable parts', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='models (DPM) use a sliding window approach where the\\nclassiﬁer is run at evenly spaced locations over the entire\\nimage [10].', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='image [10].\\nMore recent approaches like R-CNN use region proposal\\n1. Resize image.\\n2. Run convolutional network.3. Non-max suppression.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='Dog: 0.30Person: 0.64Horse: 0.28Figure 1: The YOLO Detection System. Processing images', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='with YOLO is simple and straightforward. Our system (1) resizes\\nthe input image to 448×448, (2) runs a single convolutional net-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='work on the image, and (3) thresholds the resulting detections by\\nthe model’s conﬁdence.\\nmethods to ﬁrst generate potential bounding boxes in an im-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='age and then run a classiﬁer on these proposed boxes. After\\nclassiﬁcation, post-processing is used to reﬁne the bound-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='ing boxes, eliminate duplicate detections, and rescore the\\nboxes based on other objects in the scene [13]. These com-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='plex pipelines are slow and hard to optimize because each\\nindividual component must be trained separately.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='We reframe object detection as a single regression prob-\\nlem, straight from image pixels to bounding box coordi-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='nates and class probabilities. Using our system, you only\\nlook once (YOLO) at an image to predict what objects are\\npresent and where they are.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='YOLO is refreshingly simple: see Figure 1. A sin-\\ngle convolutional network simultaneously predicts multi-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='ple bounding boxes and class probabilities for those boxes.\\nYOLO trains on full images and directly optimizes detec-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='tion performance. This uniﬁed model has several beneﬁts\\nover traditional methods of object detection.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='First, YOLO is extremely fast. Since we frame detection\\nas a regression problem we don’t need a complex pipeline.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='We simply run our neural network on a new image at test\\ntime to predict detections. Our base network runs at 45', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='frames per second with no batch processing on a Titan X\\nGPU and a fast version runs at more than 150 fps. This', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='means we can process streaming video in real-time with\\nless than 25 milliseconds of latency. Furthermore, YOLO', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='achieves more than twice the mean average precision of\\nother real-time systems. For a demo of our system running', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='in real-time on a webcam please see our project webpage:\\nhttp://pjreddie.com/yolo/ .\\nSecond, YOLO reasons globally about the image when', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='1arXiv:1506.02640v5  [cs.CV]  9 May 2016', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 0}),\n",
       " Document(page_content='making predictions. Unlike sliding window and region\\nproposal-based techniques, YOLO sees the entire image', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='during training and test time so it implicitly encodes contex-\\ntual information about classes as well as their appearance.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='Fast R-CNN, a top detection method [14], mistakes back-\\nground patches in an image for objects because it can’t see', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='the larger context. YOLO makes less than half the number\\nof background errors compared to Fast R-CNN.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='Third, YOLO learns generalizable representations of ob-\\njects. When trained on natural images and tested on art-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='work, YOLO outperforms top detection methods like DPM\\nand R-CNN by a wide margin. Since YOLO is highly gen-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='eralizable it is less likely to break down when applied to\\nnew domains or unexpected inputs.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='YOLO still lags behind state-of-the-art detection systems\\nin accuracy. While it can quickly identify objects in im-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='ages it struggles to precisely localize some objects, espe-\\ncially small ones. We examine these tradeoffs further in our\\nexperiments.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='experiments.\\nAll of our training and testing code is open source. A\\nvariety of pretrained models are also available to download.\\n2. Uniﬁed Detection', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='2. Uniﬁed Detection\\nWe unify the separate components of object detection\\ninto a single neural network. Our network uses features', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='from the entire image to predict each bounding box. It also\\npredicts all bounding boxes across all classes for an im-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='age simultaneously. This means our network reasons glob-\\nally about the full image and all the objects in the image.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='The YOLO design enables end-to-end training and real-\\ntime speeds while maintaining high average precision.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='Our system divides the input image into an S×Sgrid.\\nIf the center of an object falls into a grid cell, that grid cell', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='is responsible for detecting that object.\\nEach grid cell predicts Bbounding boxes and conﬁdence', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='scores for those boxes. These conﬁdence scores reﬂect how\\nconﬁdent the model is that the box contains an object and', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='also how accurate it thinks the box is that it predicts. For-\\nmally we deﬁne conﬁdence as Pr(Object )∗IOUtruth\\npred. If no', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='pred. If no\\nobject exists in that cell, the conﬁdence scores should be\\nzero. Otherwise we want the conﬁdence score to equal the', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='intersection over union (IOU) between the predicted box\\nand the ground truth.\\nEach bounding box consists of 5 predictions: x,y,w,h,', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='and conﬁdence. The (x,y)coordinates represent the center\\nof the box relative to the bounds of the grid cell. The width', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='and height are predicted relative to the whole image. Finally\\nthe conﬁdence prediction represents the IOU between the', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='predicted box and any ground truth box.\\nEach grid cell also predicts Cconditional class proba-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='bilities, Pr(Classi|Object ). These probabilities are condi-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='tioned on the grid cell containing an object. We only predictone set of class probabilities per grid cell, regardless of the\\nnumber of boxes B.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='number of boxes B.\\nAt test time we multiply the conditional class probabili-\\nties and the individual box conﬁdence predictions,', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='Pr(Classi|Object )∗Pr(Object )∗IOUtruth\\npred= Pr( Classi)∗IOUtruth\\npred(1)\\nwhich gives us class-speciﬁc conﬁdence scores for each', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='box. These scores encode both the probability of that class\\nappearing in the box and how well the predicted box ﬁts the\\nobject.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='object.\\nS × S grid on inputBounding boxes + confidence\\nClass probability mapFinal detections', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='Figure 2: The Model. Our system models detection as a regres-\\nsion problem. It divides the image into an S×Sgrid and for each', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='grid cell predicts Bbounding boxes, conﬁdence for those boxes,\\nandCclass probabilities. These predictions are encoded as an\\nS×S×(B∗5 +C)tensor.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='S×S×(B∗5 +C)tensor.\\nFor evaluating YOLO on P ASCAL VOC, we use S= 7,\\nB= 2. PASCAL VOC has 20 labelled classes so C= 20 .', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='Our ﬁnal prediction is a 7×7×30tensor.\\n2.1. Network Design\\nWe implement this model as a convolutional neural net-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='work and evaluate it on the P ASCAL VOC detection dataset\\n[9]. The initial convolutional layers of the network extract', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='features from the image while the fully connected layers\\npredict the output probabilities and coordinates.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='Our network architecture is inspired by the GoogLeNet\\nmodel for image classiﬁcation [34]. Our network has 24', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='convolutional layers followed by 2 fully connected layers.\\nInstead of the inception modules used by GoogLeNet, we', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='simply use 1×1reduction layers followed by 3×3convo-\\nlutional layers, similar to Lin et al [22]. The full network is\\nshown in Figure 3.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='shown in Figure 3.\\nWe also train a fast version of YOLO designed to push\\nthe boundaries of fast object detection. Fast YOLO uses a', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='neural network with fewer convolutional layers (9 instead\\nof 24) and fewer ﬁlters in those layers. Other than the size', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='of the network, all training and testing parameters are the\\nsame between YOLO and Fast YOLO.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 1}),\n",
       " Document(page_content='448\\n448\\n3\\n7\\n7\\nConv. Layer\\n7x7x64-s-2\\nMaxpool Layer\\n2x2-s-2\\n3\\n3112\\n112\\n192\\n3\\n356\\n56\\n256\\nConn. Layer4096\\nConn. Layer Conv. Layer\\n3x3x192\\nMaxpool Layer', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='Maxpool Layer\\n2x2-s-2Conv. Layers\\n1x1x128\\n3x3x256\\n1x1x256\\n3x3x512\\nMaxpool Layer\\n2x2-s-2\\n3\\n328\\n28\\n512\\nConv. Layers\\n1x1x256\\n3x3x5121x1x512\\n3x3x1024', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='3x3x1024\\nMaxpool Layer\\n2x2-s-2\\n3\\n314\\n14\\n1024\\nConv. Layers\\n1x1x512\\n3x3x10243x3x1024\\n3x3x1024-s-2\\n3\\n37\\n7\\n10247\\n7\\n10247\\n7\\n30\\n} ×4 } ×2Conv. Layers', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='3x3x1024\\n3x3x1024Figure 3: The Architecture. Our detection network has 24 convolutional layers followed by 2 fully connected layers. Alternating 1×1', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='convolutional layers reduce the features space from preceding layers. We pretrain the convolutional layers on the ImageNet classiﬁcation', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='task at half the resolution ( 224×224input image) and then double the resolution for detection.\\nThe ﬁnal output of our network is the 7×7×30tensor', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='of predictions.\\n2.2. Training\\nWe pretrain our convolutional layers on the ImageNet\\n1000-class competition dataset [30]. For pretraining we use', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='the ﬁrst 20 convolutional layers from Figure 3 followed by a\\naverage-pooling layer and a fully connected layer. We train', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='this network for approximately a week and achieve a single\\ncrop top-5 accuracy of 88% on the ImageNet 2012 valida-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='tion set, comparable to the GoogLeNet models in Caffe’s\\nModel Zoo [24]. We use the Darknet framework for all\\ntraining and inference [26].', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='We then convert the model to perform detection. Ren et\\nal. show that adding both convolutional and connected lay-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='ers to pretrained networks can improve performance [29].\\nFollowing their example, we add four convolutional lay-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='ers and two fully connected layers with randomly initialized\\nweights. Detection often requires ﬁne-grained visual infor-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='mation so we increase the input resolution of the network\\nfrom 224×224to448×448.\\nOur ﬁnal layer predicts both class probabilities and', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='bounding box coordinates. We normalize the bounding box\\nwidth and height by the image width and height so that they', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='fall between 0 and 1. We parametrize the bounding box x\\nandycoordinates to be offsets of a particular grid cell loca-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='tion so they are also bounded between 0 and 1.\\nWe use a linear activation function for the ﬁnal layer and', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='all other layers use the following leaky rectiﬁed linear acti-\\nvation:\\nφ(x) ={\\nx, ifx>0\\n0.1x,otherwise(2)', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='0.1x,otherwise(2)\\nWe optimize for sum-squared error in the output of ourmodel. We use sum-squared error because it is easy to op-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='timize, however it does not perfectly align with our goal of\\nmaximizing average precision. It weights localization er-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='ror equally with classiﬁcation error which may not be ideal.\\nAlso, in every image many grid cells do not contain any', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='object. This pushes the “conﬁdence” scores of those cells\\ntowards zero, often overpowering the gradient from cells', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='that do contain objects. This can lead to model instability,\\ncausing training to diverge early on.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='To remedy this, we increase the loss from bounding box\\ncoordinate predictions and decrease the loss from conﬁ-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='dence predictions for boxes that don’t contain objects. We\\nuse two parameters, λcoordandλnoobjto accomplish this. We\\nsetλcoord= 5andλnoobj=.5.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='Sum-squared error also equally weights errors in large\\nboxes and small boxes. Our error metric should reﬂect that', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='small deviations in large boxes matter less than in small\\nboxes. To partially address this we predict the square root', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='of the bounding box width and height instead of the width\\nand height directly.\\nYOLO predicts multiple bounding boxes per grid cell.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='At training time we only want one bounding box predictor\\nto be responsible for each object. We assign one predictor', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='to be “responsible” for predicting an object based on which\\nprediction has the highest current IOU with the ground', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='truth. This leads to specialization between the bounding box\\npredictors. Each predictor gets better at predicting certain', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='sizes, aspect ratios, or classes of object, improving overall\\nrecall.\\nDuring training we optimize the following, multi-part', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 2}),\n",
       " Document(page_content='loss function:\\nλcoordS2∑\\ni=0B∑\\nj=01obj\\nij[\\n(xi−ˆxi)2+ (yi−ˆyi)2]\\n+λcoordS2∑\\ni=0B∑\\nj=01obj\\nij[(√wi−√\\nˆwi)2+(√\\nhi−√\\nˆhi)2]\\n+S2∑\\ni=0B∑\\nj=01obj\\nij(', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='i=0B∑\\nj=01obj\\nij(\\nCi−ˆCi)2\\n+λnoobjS2∑\\ni=0B∑\\nj=01noobj\\nij(\\nCi−ˆCi)2\\n+S2∑\\ni=01obj\\ni∑\\nc∈classes(pi(c)−ˆpi(c))2(3)\\nwhere 1obj', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='where 1obj\\nidenotes if object appears in cell iand 1obj\\nijde-\\nnotes that the jth bounding box predictor in cell iis “re-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='sponsible” for that prediction.\\nNote that the loss function only penalizes classiﬁcation', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='error if an object is present in that grid cell (hence the con-\\nditional class probability discussed earlier). It also only pe-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='nalizes bounding box coordinate error if that predictor is\\n“responsible” for the ground truth box (i.e. has the highest', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='IOU of any predictor in that grid cell).\\nWe train the network for about 135 epochs on the train-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='ing and validation data sets from P ASCAL VOC 2007 and\\n2012. When testing on 2012 we also include the VOC 2007', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='test data for training. Throughout training we use a batch\\nsize of 64, a momentum of 0.9and a decay of 0.0005 .', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='Our learning rate schedule is as follows: For the ﬁrst\\nepochs we slowly raise the learning rate from 10−3to10−2.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='If we start at a high learning rate our model often diverges\\ndue to unstable gradients. We continue training with 10−2', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='for 75 epochs, then 10−3for 30 epochs, and ﬁnally 10−4\\nfor 30 epochs.\\nTo avoid overﬁtting we use dropout and extensive data', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='augmentation. A dropout layer with rate = .5 after the ﬁrst\\nconnected layer prevents co-adaptation between layers [18].', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='For data augmentation we introduce random scaling and\\ntranslations of up to 20% of the original image size. We', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='also randomly adjust the exposure and saturation of the im-\\nage by up to a factor of 1.5in the HSV color space.\\n2.3. Inference', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='2.3. Inference\\nJust like in training, predicting detections for a test image\\nonly requires one network evaluation. On P ASCAL VOC the', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='network predicts 98 bounding boxes per image and class\\nprobabilities for each box. YOLO is extremely fast at test', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='time since it only requires a single network evaluation, un-\\nlike classiﬁer-based methods.\\nThe grid design enforces spatial diversity in the bound-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='ing box predictions. Often it is clear which grid cell an\\nobject falls in to and the network only predicts one box for', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='each object. However, some large objects or objects nearthe border of multiple cells can be well localized by multi-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='ple cells. Non-maximal suppression can be used to ﬁx these\\nmultiple detections. While not critical to performance as it', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='is for R-CNN or DPM, non-maximal suppression adds 2-\\n3% in mAP.\\n2.4. Limitations of YOLO\\nYOLO imposes strong spatial constraints on bounding', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='box predictions since each grid cell only predicts two boxes\\nand can only have one class. This spatial constraint lim-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='its the number of nearby objects that our model can pre-\\ndict. Our model struggles with small objects that appear in\\ngroups, such as ﬂocks of birds.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='Since our model learns to predict bounding boxes from\\ndata, it struggles to generalize to objects in new or unusual', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='aspect ratios or conﬁgurations. Our model also uses rela-\\ntively coarse features for predicting bounding boxes since', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='our architecture has multiple downsampling layers from the\\ninput image.\\nFinally, while we train on a loss function that approxi-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='mates detection performance, our loss function treats errors\\nthe same in small bounding boxes versus large bounding', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='boxes. A small error in a large box is generally benign but a\\nsmall error in a small box has a much greater effect on IOU.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='Our main source of error is incorrect localizations.\\n3. Comparison to Other Detection Systems\\nObject detection is a core problem in computer vision.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='Detection pipelines generally start by extracting a set of\\nrobust features from input images (Haar [25], SIFT [23],', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='HOG [4], convolutional features [6]). Then, classiﬁers\\n[36, 21, 13, 10] or localizers [1, 32] are used to identify', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='objects in the feature space. These classiﬁers or localizers\\nare run either in sliding window fashion over the whole im-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='age or on some subset of regions in the image [35, 15, 39].\\nWe compare the YOLO detection system to several top de-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='tection frameworks, highlighting key similarities and differ-\\nences.\\nDeformable parts models. Deformable parts models', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='(DPM) use a sliding window approach to object detection\\n[10]. DPM uses a disjoint pipeline to extract static features,', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='classify regions, predict bounding boxes for high scoring\\nregions, etc. Our system replaces all of these disparate parts', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='with a single convolutional neural network. The network\\nperforms feature extraction, bounding box prediction, non-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='maximal suppression, and contextual reasoning all concur-\\nrently. Instead of static features, the network trains the fea-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='tures in-line and optimizes them for the detection task. Our\\nuniﬁed architecture leads to a faster, more accurate model\\nthan DPM.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='than DPM.\\nR-CNN. R-CNN and its variants use region proposals in-\\nstead of sliding windows to ﬁnd objects in images. Selective', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 3}),\n",
       " Document(page_content='Search [35] generates potential bounding boxes, a convolu-\\ntional network extracts features, an SVM scores the boxes, a', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='linear model adjusts the bounding boxes, and non-max sup-\\npression eliminates duplicate detections. Each stage of this', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='complex pipeline must be precisely tuned independently\\nand the resulting system is very slow, taking more than 40', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='seconds per image at test time [14].\\nYOLO shares some similarities with R-CNN. Each grid\\ncell proposes potential bounding boxes and scores those', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='boxes using convolutional features. However, our system\\nputs spatial constraints on the grid cell proposals which', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='helps mitigate multiple detections of the same object. Our\\nsystem also proposes far fewer bounding boxes, only 98', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='per image compared to about 2000 from Selective Search.\\nFinally, our system combines these individual components', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='into a single, jointly optimized model.\\nOther Fast Detectors Fast and Faster R-CNN focus on\\nspeeding up the R-CNN framework by sharing computa-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='tion and using neural networks to propose regions instead\\nof Selective Search [14] [28]. While they offer speed and', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='accuracy improvements over R-CNN, both still fall short of\\nreal-time performance.\\nMany research efforts focus on speeding up the DPM', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='pipeline [31] [38] [5]. They speed up HOG computation,\\nuse cascades, and push computation to GPUs. However,', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='only 30Hz DPM [31] actually runs in real-time.\\nInstead of trying to optimize individual components of', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='a large detection pipeline, YOLO throws out the pipeline\\nentirely and is fast by design.\\nDetectors for single classes like faces or people can be', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='highly optimized since they have to deal with much less\\nvariation [37]. YOLO is a general purpose detector that', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='learns to detect a variety of objects simultaneously.\\nDeep MultiBox. Unlike R-CNN, Szegedy et al. train a', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='convolutional neural network to predict regions of interest\\n[8] instead of using Selective Search. MultiBox can also', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='perform single object detection by replacing the conﬁdence\\nprediction with a single class prediction. However, Multi-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='Box cannot perform general object detection and is still just\\na piece in a larger detection pipeline, requiring further im-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='age patch classiﬁcation. Both YOLO and MultiBox use a\\nconvolutional network to predict bounding boxes in an im-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='age but YOLO is a complete detection system.\\nOverFeat. Sermanet et al. train a convolutional neural', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='network to perform localization and adapt that localizer to\\nperform detection [32]. OverFeat efﬁciently performs slid-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='ing window detection but it is still a disjoint system. Over-\\nFeat optimizes for localization, not detection performance.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='Like DPM, the localizer only sees local information when\\nmaking a prediction. OverFeat cannot reason about global', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='context and thus requires signiﬁcant post-processing to pro-\\nduce coherent detections.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='MultiGrasp. Our work is similar in design to work ongrasp detection by Redmon et al [27]. Our grid approach to', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='bounding box prediction is based on the MultiGrasp system\\nfor regression to grasps. However, grasp detection is a much', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='simpler task than object detection. MultiGrasp only needs\\nto predict a single graspable region for an image containing', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='one object. It doesn’t have to estimate the size, location,\\nor boundaries of the object or predict it’s class, only ﬁnd a', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='region suitable for grasping. YOLO predicts both bounding\\nboxes and class probabilities for multiple objects of multi-\\nple classes in an image.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='4. Experiments\\nFirst we compare YOLO with other real-time detection\\nsystems on P ASCAL VOC 2007. To understand the differ-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='ences between YOLO and R-CNN variants we explore the\\nerrors on VOC 2007 made by YOLO and Fast R-CNN, one', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='of the highest performing versions of R-CNN [14]. Based\\non the different error proﬁles we show that YOLO can be', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='used to rescore Fast R-CNN detections and reduce the er-\\nrors from background false positives, giving a signiﬁcant', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='performance boost. We also present VOC 2012 results and\\ncompare mAP to current state-of-the-art methods. Finally,', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='we show that YOLO generalizes to new domains better than\\nother detectors on two artwork datasets.\\n4.1. Comparison to Other Real-Time Systems', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='Many research efforts in object detection focus on mak-\\ning standard detection pipelines fast. [5] [38] [31] [14] [17]', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='[28] However, only Sadeghi et al. actually produce a de-\\ntection system that runs in real-time (30 frames per second', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='or better) [31]. We compare YOLO to their GPU imple-\\nmentation of DPM which runs either at 30Hz or 100Hz.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='While the other efforts don’t reach the real-time milestone\\nwe also compare their relative mAP and speed to examine', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='the accuracy-performance tradeoffs available in object de-\\ntection systems.\\nFast YOLO is the fastest object detection method on', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='PASCAL ; as far as we know, it is the fastest extant object\\ndetector. With 52.7%mAP, it is more than twice as accurate', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='as prior work on real-time detection. YOLO pushes mAP to\\n63.4%while still maintaining real-time performance.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='We also train YOLO using VGG-16. This model is more\\naccurate but also signiﬁcantly slower than YOLO. It is use-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='ful for comparison to other detection systems that rely on\\nVGG-16 but since it is slower than real-time the rest of the', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='paper focuses on our faster models.\\nFastest DPM effectively speeds up DPM without sacri-\\nﬁcing much mAP but it still misses real-time performance', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='by a factor of 2 [38]. It also is limited by DPM’s relatively\\nlow accuracy on detection compared to neural network ap-\\nproaches.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='proaches.\\nR-CNN minus R replaces Selective Search with static\\nbounding box proposals [20]. While it is much faster than', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 4}),\n",
       " Document(page_content='Real-Time Detectors Train mAP FPS\\n100Hz DPM [31] 2007 16.0 100\\n30Hz DPM [31] 2007 26.1 30\\nFast YOLO 2007+2012 52.7 155\\nYOLO 2007+2012 63.4 45', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='Less Than Real-Time\\nFastest DPM [38] 2007 30.4 15\\nR-CNN Minus R [20] 2007 53.5 6\\nFast R-CNN [14] 2007+2012 70.0 0.5', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='Faster R-CNN VGG-16[28] 2007+2012 73.2 7\\nFaster R-CNN ZF [28] 2007+2012 62.1 18\\nYOLO VGG-16 2007+2012 66.4 21', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='Table 1: Real-Time Systems on P ASCAL VOC 2007. Compar-\\ning the performance and speed of fast detectors. Fast YOLO is', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='the fastest detector on record for P ASCAL VOC detection and is\\nstill twice as accurate as any other real-time detector. YOLO is', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='10 mAP more accurate than the fast version while still well above\\nreal-time in speed.\\nR-CNN, it still falls short of real-time and takes a signiﬁcant', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='accuracy hit from not having good proposals.\\nFast R-CNN speeds up the classiﬁcation stage of R-CNN', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='but it still relies on selective search which can take around\\n2 seconds per image to generate bounding box proposals.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='Thus it has high mAP but at 0.5fps it is still far from real-\\ntime.\\nThe recent Faster R-CNN replaces selective search with', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='a neural network to propose bounding boxes, similar to\\nSzegedy et al. [8] In our tests, their most accurate model', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='achieves 7 fps while a smaller, less accurate one runs at\\n18 fps. The VGG-16 version of Faster R-CNN is 10 mAP', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='higher but is also 6 times slower than YOLO. The Zeiler-\\nFergus Faster R-CNN is only 2.5 times slower than YOLO\\nbut is also less accurate.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='4.2. VOC 2007 Error Analysis\\nTo further examine the differences between YOLO and\\nstate-of-the-art detectors, we look at a detailed breakdown', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='of results on VOC 2007. We compare YOLO to Fast R-\\nCNN since Fast R-CNN is one of the highest performing', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='detectors on P ASCAL and it’s detections are publicly avail-\\nable.\\nWe use the methodology and tools of Hoiem et al. [19]', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='For each category at test time we look at the top N predic-\\ntions for that category. Each prediction is either correct or', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='it is classiﬁed based on the type of error:\\n•Correct: correct class and IOU >.5\\n•Localization: correct class, .1<IOU<.5', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='•Similar: class is similar, IOU >.1\\nCorrect: 71.6% Correct: 65.5%Loc: 8.6%Sim: 4.3%Other: 1.9%Background: 13.6%', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='Loc: 19.0%Sim: 6.75%Other: 4.0%Background: 4.75%Fast R-CNN YOLOFigure 4: Error Analysis: Fast R-CNN vs. YOLO These', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='charts show the percentage of localization and background errors\\nin the top N detections for various categories (N = # objects in that\\ncategory).', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='category).\\n•Other: class is wrong, IOU >.1\\n•Background: IOU <.1for any object\\nFigure 4 shows the breakdown of each error type aver-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='aged across all 20 classes.\\nYOLO struggles to localize objects correctly. Localiza-\\ntion errors account for more of YOLO’s errors than all other', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='sources combined. Fast R-CNN makes much fewer local-\\nization errors but far more background errors. 13.6% of', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='it’s top detections are false positives that don’t contain any\\nobjects. Fast R-CNN is almost 3x more likely to predict', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='background detections than YOLO.\\n4.3. Combining Fast R-CNN and YOLO\\nYOLO makes far fewer background mistakes than Fast', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='R-CNN. By using YOLO to eliminate background detec-\\ntions from Fast R-CNN we get a signiﬁcant boost in perfor-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='mance. For every bounding box that R-CNN predicts we\\ncheck to see if YOLO predicts a similar box. If it does, we', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='give that prediction a boost based on the probability pre-\\ndicted by YOLO and the overlap between the two boxes.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='The best Fast R-CNN model achieves a mAP of 71.8%\\non the VOC 2007 test set. When combined with YOLO, its\\nmAP Combined Gain\\nFast R-CNN 71.8 - -', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='Fast R-CNN 71.8 - -\\nFast R-CNN (2007 data) 66.9 72.4 .6\\nFast R-CNN (VGG-M) 59.2 72.4 .6\\nFast R-CNN (CaffeNet) 57.1 72.1 .3\\nYOLO 63.4 75.0 3.2', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='YOLO 63.4 75.0 3.2\\nTable 2: Model combination experiments on VOC 2007. We\\nexamine the effect of combining various models with the best ver-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='sion of Fast R-CNN. Other versions of Fast R-CNN provide only\\na small beneﬁt while YOLO provides a signiﬁcant performance\\nboost.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 5}),\n",
       " Document(page_content='VOC 2012 test mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike personplant sheep sofa train tv', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='MR CNN MORE DATA [11] 73.9 85.5 82.9 76.6 57.8 62.7 79.4 77.2 86.6 55.0 79.1 62.2 87.0 83.4 84.7 78.9 45.3 73.4 65.8 80.3 74.0', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='HyperNet VGG 71.4 84.2 78.5 73.6 55.6 53.7 78.7 79.8 87.7 49.6 74.9 52.1 86.0 81.7 83.3 81.8 48.6 73.5 59.4 79.9 65.7', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='HyperNet SP 71.3 84.1 78.3 73.3 55.5 53.6 78.6 79.6 87.5 49.5 74.9 52.1 85.6 81.6 83.2 81.6 48.4 73.2 59.3 79.7 65.6', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='Fast R-CNN + YOLO 70.7 83.4 78.5 73.5 55.8 43.4 79.1 73.1 89.4 49.4 75.5 57.0 87.5 80.9 81.0 74.7 41.8 71.5 68.5 82.1 67.2', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='MR CNN SCNN [11] 70.7 85.0 79.6 71.5 55.3 57.7 76.0 73.9 84.6 50.5 74.3 61.7 85.5 79.9 81.7 76.4 41.0 69.0 61.2 77.7 72.1', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='Faster R-CNN [28] 70.4 84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='DEEP ENS COCO 70.1 84.0 79.4 71.6 51.9 51.1 74.1 72.1 88.6 48.3 73.4 57.8 86.1 80.0 80.7 70.4 46.6 69.6 68.8 75.9 71.4', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='NoC [29] 68.8 82.8 79.0 71.6 52.3 53.7 74.1 69.0 84.9 46.9 74.3 53.1 85.0 81.3 79.5 72.2 38.9 72.4 59.5 76.7 68.1', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='Fast R-CNN [14] 68.4 82.3 78.4 70.8 52.3 38.7 77.8 71.6 89.3 44.2 73.0 55.0 87.5 80.5 80.8 72.0 35.1 68.3 65.7 80.4 64.2', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='UMICH FGS STRUCT 66.4 82.9 76.1 64.1 44.6 49.4 70.3 71.2 84.6 42.7 68.6 55.8 82.7 77.1 79.9 68.7 41.4 69.0 60.0 72.0 66.2', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='NUS NIN C2000 [7] 63.8 80.2 73.8 61.9 43.7 43.0 70.3 67.6 80.7 41.9 69.7 51.7 78.2 75.2 76.9 65.1 38.6 68.3 58.0 68.7 63.3', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='BabyLearning [7] 63.2 78.0 74.2 61.3 45.7 42.7 68.2 66.8 80.2 40.6 70.0 49.8 79.0 74.5 77.9 64.0 35.3 67.9 55.7 68.7 62.6', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='NUS NIN 62.4 77.9 73.1 62.6 39.5 43.3 69.1 66.4 78.9 39.1 68.1 50.0 77.2 71.3 76.1 64.7 38.4 66.9 56.2 66.9 62.7', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='R-CNN VGG BB [13] 62.4 79.6 72.7 61.9 41.2 41.9 65.9 66.4 84.6 38.5 67.2 46.7 82.0 74.8 76.0 65.2 35.6 65.4 54.2 67.4 60.3', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='R-CNN VGG [13] 59.2 76.8 70.9 56.6 37.5 36.9 62.9 63.6 81.1 35.7 64.3 43.9 80.4 71.6 74.0 60.0 30.8 63.4 52.0 63.5 58.7', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='YOLO 57.9 77.0 67.2 57.7 38.3 22.7 68.3 55.9 81.4 36.2 60.8 48.5 77.2 72.3 71.3 63.5 28.9 52.2 54.8 73.9 50.8', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='Feature Edit [33] 56.3 74.6 69.1 54.4 39.1 33.1 65.2 62.7 69.7 30.8 56.0 44.6 70.0 64.4 71.1 60.2 33.3 61.3 46.4 61.7 57.8', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='R-CNN BB [13] 53.3 71.8 65.8 52.0 34.1 32.6 59.6 60.0 69.8 27.6 52.0 41.7 69.6 61.3 68.3 57.8 29.6 57.8 40.9 59.3 54.1', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='SDS [16] 50.7 69.7 58.4 48.5 28.3 28.8 61.3 57.5 70.8 24.1 50.7 35.9 64.9 59.1 65.8 57.1 26.0 58.8 38.6 58.9 50.7', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='R-CNN [13] 49.6 68.1 63.8 46.1 29.4 27.9 56.6 57.0 65.9 26.5 48.7 39.5 66.2 57.3 65.4 53.2 26.2 54.5 38.1 50.6 51.6', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='Table 3: PASCAL VOC 2012 Leaderboard. YOLO compared with the full comp4 (outside data allowed) public leaderboard as of', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='November 6th, 2015. Mean average precision and per-class average precision are shown for a variety of detection methods. YOLO is the', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='only real-time detector. Fast R-CNN + YOLO is the forth highest scoring method, with a 2.3% boost over Fast R-CNN.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='mAP increases by 3.2% to 75.0%. We also tried combining\\nthe top Fast R-CNN model with several other versions of', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='Fast R-CNN. Those ensembles produced small increases in\\nmAP between .3 and .6%, see Table 2 for details.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='The boost from YOLO is not simply a byproduct of\\nmodel ensembling since there is little beneﬁt from combin-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='ing different versions of Fast R-CNN. Rather, it is precisely\\nbecause YOLO makes different kinds of mistakes at test', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='time that it is so effective at boosting Fast R-CNN’s per-\\nformance.\\nUnfortunately, this combination doesn’t beneﬁt from the', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='speed of YOLO since we run each model seperately and\\nthen combine the results. However, since YOLO is so fast', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='it doesn’t add any signiﬁcant computational time compared\\nto Fast R-CNN.\\n4.4. VOC 2012 Results\\nOn the VOC 2012 test set, YOLO scores 57.9% mAP.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='This is lower than the current state of the art, closer to\\nthe original R-CNN using VGG-16, see Table 3. Our sys-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='tem struggles with small objects compared to its closest\\ncompetitors. On categories like bottle ,sheep , and', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='tv/monitor YOLO scores 8-10% lower than R-CNN or\\nFeature Edit. However, on other categories like cat and\\ntrain YOLO achieves higher performance.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='Our combined Fast R-CNN + YOLO model is one of the\\nhighest performing detection methods. Fast R-CNN gets', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='a 2.3% improvement from the combination with YOLO,\\nboosting it 5 spots up on the public leaderboard.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='4.5. Generalizability: Person Detection in Artwork\\nAcademic datasets for object detection draw the training', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='and testing data from the same distribution. In real-world', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='applications it is hard to predict all possible use cases andthe test data can diverge from what the system has seen be-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='fore [3]. We compare YOLO to other detection systems on\\nthe Picasso Dataset [12] and the People-Art Dataset [3], two', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='datasets for testing person detection on artwork.\\nFigure 5 shows comparative performance between', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='YOLO and other detection methods. For reference, we give\\nVOC 2007 detection AP on person where all models are', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='trained only on VOC 2007 data. On Picasso models are\\ntrained on VOC 2012 while on People-Art they are trained\\non VOC 2010.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='on VOC 2010.\\nR-CNN has high AP on VOC 2007. However, R-CNN\\ndrops off considerably when applied to artwork. R-CNN', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='uses Selective Search for bounding box proposals which is\\ntuned for natural images. The classiﬁer step in R-CNN only', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='sees small regions and needs good proposals.\\nDPM maintains its AP well when applied to artwork.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='Prior work theorizes that DPM performs well because it has\\nstrong spatial models of the shape and layout of objects.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='Though DPM doesn’t degrade as much as R-CNN, it starts\\nfrom a lower AP.\\nYOLO has good performance on VOC 2007 and its AP', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='degrades less than other methods when applied to artwork.\\nLike DPM, YOLO models the size and shape of objects,', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='as well as relationships between objects and where objects\\ncommonly appear. Artwork and natural images are very', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='different on a pixel level but they are similar in terms of\\nthe size and shape of objects, thus YOLO can still predict', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='good bounding boxes and detections.\\n5. Real-Time Detection In The Wild\\nYOLO is a fast, accurate object detector, making it ideal', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='for computer vision applications. We connect YOLO to a\\nwebcam and verify that it maintains real-time performance,', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 6}),\n",
       " Document(page_content='Poselets\\nRCNN\\nD&THumans\\nDPMYOLO\\n(a)Picasso Dataset precision-recall curves.VOC 2007 Picasso People-Art\\nAP AP BestF1 AP\\nYOLO 59.2 53.3 0.590 45', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='R-CNN 54.2 10.4 0.226 26\\nDPM 43.2 37.8 0.458 32\\nPoselets [2] 36.5 17.8 0.271\\nD&T [4] - 1.9 0.051', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='D&T [4] - 1.9 0.051\\n(b)Quantitative results on the VOC 2007, Picasso, and People-Art Datasets.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='The Picasso Dataset evaluates on both AP and best F1score.\\nFigure 5: Generalization results on Picasso and People-Art datasets.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='Figure 6: Qualitative Results. YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='does think one person is an airplane.\\nincluding the time to fetch images from the camera and dis-\\nplay the detections.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='The resulting system is interactive and engaging. While\\nYOLO processes images individually, when attached to a', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='webcam it functions like a tracking system, detecting ob-\\njects as they move around and change in appearance. A', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='demo of the system and the source code can be found on\\nour project website: http://pjreddie.com/yolo/ .\\n6. Conclusion', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='6. Conclusion\\nWe introduce YOLO, a uniﬁed model for object detec-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='tion. Our model is simple to construct and can be traineddirectly on full images. Unlike classiﬁer-based approaches,', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='YOLO is trained on a loss function that directly corresponds\\nto detection performance and the entire model is trained\\njointly.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='jointly.\\nFast YOLO is the fastest general-purpose object detec-\\ntor in the literature and YOLO pushes the state-of-the-art in', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='real-time object detection. YOLO also generalizes well to\\nnew domains making it ideal for applications that rely on\\nfast, robust object detection.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='Acknowledgements: This work is partially supported by\\nONR N00014-13-1-0720, NSF IIS-1338054, and The Allen\\nDistinguished Investigator Award.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 7}),\n",
       " Document(page_content='References\\n[1] M. B. Blaschko and C. H. Lampert. Learning to localize ob-\\njects with structured output regression. In Computer Vision–', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='ECCV 2008 , pages 2–15. Springer, 2008. 4\\n[2] L. Bourdev and J. Malik. Poselets: Body part detectors', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='trained using 3d human pose annotations. In International\\nConference on Computer Vision (ICCV) , 2009. 8', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='[3] H. Cai, Q. Wu, T. Corradi, and P. Hall. The cross-\\ndepiction problem: Computer vision algorithms for recog-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='nising objects in artwork and in photographs. arXiv preprint\\narXiv:1505.00110 , 2015. 7', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='[4] N. Dalal and B. Triggs. Histograms of oriented gradients for\\nhuman detection. In Computer Vision and Pattern Recogni-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='tion, 2005. CVPR 2005. IEEE Computer Society Conference\\non, volume 1, pages 886–893. IEEE, 2005. 4, 8', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='[5] T. Dean, M. Ruzon, M. Segal, J. Shlens, S. Vijaya-\\nnarasimhan, J. Yagnik, et al. Fast, accurate detection of', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='100,000 object classes on a single machine. In Computer\\nVision and Pattern Recognition (CVPR), 2013 IEEE Confer-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='ence on , pages 1814–1821. IEEE, 2013. 5\\n[6] J. Donahue, Y . Jia, O. Vinyals, J. Hoffman, N. Zhang,', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='E. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-\\nvation feature for generic visual recognition. arXiv preprint\\narXiv:1310.1531 , 2013. 4', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='[7] J. Dong, Q. Chen, S. Yan, and A. Yuille. Towards uniﬁed\\nobject detection and semantic segmentation. In Computer', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='Vision–ECCV 2014 , pages 299–314. Springer, 2014. 7\\n[8] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='object detection using deep neural networks. In Computer\\nVision and Pattern Recognition (CVPR), 2014 IEEE Confer-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='ence on , pages 2155–2162. IEEE, 2014. 5, 6\\n[9] M. Everingham, S. M. A. Eslami, L. Van Gool, C. K. I.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='Williams, J. Winn, and A. Zisserman. The pascal visual ob-\\nject classes challenge: A retrospective. International Journal', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='of Computer Vision , 111(1):98–136, Jan. 2015. 2\\n[10] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='manan. Object detection with discriminatively trained part\\nbased models. IEEE Transactions on Pattern Analysis and', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='Machine Intelligence , 32(9):1627–1645, 2010. 1, 4\\n[11] S. Gidaris and N. Komodakis. Object detection via a multi-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='region & semantic segmentation-aware CNN model. CoRR ,\\nabs/1505.01749, 2015. 7\\n[12] S. Ginosar, D. Haas, T. Brown, and J. Malik. Detecting peo-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='ple in cubist art. In Computer Vision-ECCV 2014 Workshops ,\\npages 101–116. Springer, 2014. 7', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\\nture hierarchies for accurate object detection and semantic', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='segmentation. In Computer Vision and Pattern Recognition\\n(CVPR), 2014 IEEE Conference on , pages 580–587. IEEE,\\n2014. 1, 4, 7', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='2014. 1, 4, 7\\n[14] R. B. Girshick. Fast R-CNN. CoRR , abs/1504.08083, 2015.\\n2, 5, 6, 7\\n[15] S. Gould, T. Gao, and D. Koller. Region-based segmenta-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='tion and object detection. In Advances in neural information', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='processing systems , pages 655–663, 2009. 4[16] B. Hariharan, P. Arbel ´aez, R. Girshick, and J. Malik. Simul-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='taneous detection and segmentation. In Computer Vision–\\nECCV 2014 , pages 297–312. Springer, 2014. 7', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='[17] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\\nin deep convolutional networks for visual recognition. arXiv', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='preprint arXiv:1406.4729 , 2014. 5\\n[18] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='R. R. Salakhutdinov. Improving neural networks by pre-\\nventing co-adaptation of feature detectors. arXiv preprint\\narXiv:1207.0580 , 2012. 4', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='[19] D. Hoiem, Y . Chodpathumwan, and Q. Dai. Diagnosing error\\nin object detectors. In Computer Vision–ECCV 2012 , pages\\n340–353. Springer, 2012. 6', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='[20] K. Lenc and A. Vedaldi. R-cnn minus r. arXiv preprint\\narXiv:1506.06981 , 2015. 5, 6', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='[21] R. Lienhart and J. Maydt. An extended set of haar-like fea-\\ntures for rapid object detection. In Image Processing. 2002.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='Proceedings. 2002 International Conference on , volume 1,\\npages I–900. IEEE, 2002. 4\\n[22] M. Lin, Q. Chen, and S. Yan. Network in network. CoRR ,', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='abs/1312.4400, 2013. 2\\n[23] D. G. Lowe. Object recognition from local scale-invariant\\nfeatures. In Computer vision, 1999. The proceedings of the', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='seventh IEEE international conference on , volume 2, pages\\n1150–1157. Ieee, 1999. 4\\n[24] D. Mishkin. Models accuracy on imagenet 2012', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='val. https://github.com/BVLC/caffe/wiki/\\nModels-accuracy-on-ImageNet-2012-val . Ac-\\ncessed: 2015-10-2. 3', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='[25] C. P. Papageorgiou, M. Oren, and T. Poggio. A general\\nframework for object detection. In Computer vision, 1998.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='sixth international conference on , pages 555–562. IEEE,\\n1998. 4\\n[26] J. Redmon. Darknet: Open source neural networks in c.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='http://pjreddie.com/darknet/ , 2013–2016. 3\\n[27] J. Redmon and A. Angelova. Real-time grasp detection using', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='convolutional neural networks. CoRR , abs/1412.3128, 2014.\\n5\\n[28] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: To-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='wards real-time object detection with region proposal net-\\nworks. arXiv preprint arXiv:1506.01497 , 2015. 5, 6, 7', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='[29] S. Ren, K. He, R. B. Girshick, X. Zhang, and J. Sun. Object\\ndetection networks on convolutional feature maps. CoRR ,\\nabs/1504.06066, 2015. 3, 7', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='[30] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\\nRecognition Challenge. International Journal of Computer\\nVision (IJCV) , 2015. 3', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='[31] M. A. Sadeghi and D. Forsyth. 30hz object detection with\\ndpm v5. In Computer Vision–ECCV 2014 , pages 65–79.\\nSpringer, 2014. 5, 6', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='[32] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\\nand Y . LeCun. Overfeat: Integrated recognition, localiza-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='tion and detection using convolutional networks. CoRR ,\\nabs/1312.6229, 2013. 4, 5', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 8}),\n",
       " Document(page_content='[33] Z. Shen and X. Xue. Do more dropouts in pool5 feature maps\\nfor better object detection. arXiv preprint arXiv:1409.6911 ,\\n2014. 7', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 9}),\n",
       " Document(page_content='2014. 7\\n[34] C. Szegedy, W. Liu, Y . Jia, P. Sermanet, S. Reed,\\nD. Anguelov, D. Erhan, V . Vanhoucke, and A. Rabinovich.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 9}),\n",
       " Document(page_content='Going deeper with convolutions. CoRR , abs/1409.4842,\\n2014. 2\\n[35] J. R. Uijlings, K. E. van de Sande, T. Gevers, and A. W.', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 9}),\n",
       " Document(page_content='Smeulders. Selective search for object recognition. Inter-\\nnational journal of computer vision , 104(2):154–171, 2013.\\n4', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 9}),\n",
       " Document(page_content='4\\n[36] P. Viola and M. Jones. Robust real-time object detection.\\nInternational Journal of Computer Vision , 4:34–47, 2001. 4', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 9}),\n",
       " Document(page_content='[37] P. Viola and M. J. Jones. Robust real-time face detection.\\nInternational journal of computer vision , 57(2):137–154,\\n2004. 5', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 9}),\n",
       " Document(page_content='2004. 5\\n[38] J. Yan, Z. Lei, L. Wen, and S. Z. Li. The fastest deformable\\npart model for object detection. In Computer Vision and Pat-', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 9}),\n",
       " Document(page_content='tern Recognition (CVPR), 2014 IEEE Conference on , pages\\n2497–2504. IEEE, 2014. 5, 6', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 9}),\n",
       " Document(page_content='[39] C. L. Zitnick and P. Doll ´ar. Edge boxes: Locating object pro-\\nposals from edges. In Computer Vision–ECCV 2014 , pages', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 9}),\n",
       " Document(page_content='391–405. Springer, 2014. 4', metadata={'source': 'PDFs\\\\yolo.pdf', 'page': 9})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You Only Look Once:\n",
      "Uniﬁed, Real-Time Object Detection\n",
      "Joseph Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗†\n"
     ]
    }
   ],
   "source": [
    "print(test_chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "University of Washington∗, Allen Institute for AI†, Facebook AI Research¶\n",
      "http://pjreddie.com/yolo/\n",
      "Abstract\n"
     ]
    }
   ],
   "source": [
    "print(test_chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajays\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "embeddings=OpenAIEmbeddings(api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001A22F5F0A90>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001A22F5F8DD0>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='sk-proj-8y3pt4Q1Mlu73Ty2KL4GT3BlbkFJaGhcDQ5aEZpJxl4ahITJ', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pinecone-client in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.2.2)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client) (2024.2.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client) (4.11.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pinecone-client) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ajays\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.64.1->pinecone-client) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pinecone-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"docs-quickstart-index\"\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        ) \n",
    "    ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore(index_name=index_name, embedding=embeddings)\n",
    "docsearch=vectorstore.add_documents(test_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={'k':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x000001A22FF80990>, search_kwargs={'k': 1})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"yolo outperforms which model?\"\n",
    "docs=vectorstore.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='train YOLO achieves higher performance.', metadata={'page': 6.0, 'source': 'PDFs\\\\yolo.pdf'}),\n",
       " Document(page_content='YOLO is extremely fast at test', metadata={'page': 3.0, 'source': 'PDFs\\\\yolo.pdf'}),\n",
       " Document(page_content='work, YOLO outperforms top detection methods like', metadata={'page': 1.0, 'source': 'PDFs\\\\yolo.pdf'}),\n",
       " Document(page_content='and YOLO pushes the state-of-the-art in', metadata={'page': 7.0, 'source': 'PDFs\\\\yolo.pdf'})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm,\n",
    "                                          chain_type='stuff',\n",
    "                                          retriever=vectorstore.as_retriever(),\n",
    "                                         \n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the accuracy of yolo?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The accuracy of YOLO can vary depending on the specific implementation and dataset used for training. Generally, YOLO is known to be a fast and accurate object detector, but the exact accuracy percentage would depend on the specific factors mentioned above.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
